#!/usr/bin/env python
# coding: utf-8

# # Movie Recommender System Project | Content Based Recommender System with Heroku Deployment
# 

# ## **What is a Recommender System?**
# 
# ### A Recommender System is a software or algorithm designed to provide personalized recommendations to users. Its primary goal is to suggest items, products, or content that users are likely to be interested in based on their preferences, past behavior, and interactions with the system. Recommender Systems are widely used in various industries, such as e-commerce, entertainment, social media, and more, to enhance user experience and increase engagement.
# 
# ### Imagine you are browsing an online shopping website, and the website suggests some products that you might like based on your previous purchases or browsing history. Or think about streaming platforms like Netflix or Spotify, which recommend movies or songs based on your viewing or listening habits. These personalized suggestions are generated by Recommender Systems.
# 
# ## *How does a Recommender System work?*
# 
# ### Recommender Systems work by analyzing large amounts of data to identify patterns, similarities, and correlations between users and items (products, movies, books, etc.). There are several popular techniques for building Recommender Systems, but the two main approaches are:
# 
# ### **Content-Based Filtering:** This approach uses the attributes or features of the items and the user's historical interactions to make recommendations. It suggests items that are similar to the ones the user has shown interest in before. For example, if a user has watched and liked action movies in the past, a content-based system might recommend more action movies with similar themes, actors, or genres.
# 
# ### **Collaborative Filtering:** Collaborative filtering doesn't rely on the attributes of items but instead focuses on finding patterns in the preferences and behaviors of multiple users. It identifies users who have similar tastes and interests and recommends items that those like-minded users have enjoyed. For instance, if two users have similar movie-watching behavior and one of them has watched and rated a particular movie highly, the system may recommend that movie to the other user.
# 
# ### Both approaches have their strengths and weaknesses, and some Recommender Systems combine both methods to provide more accurate and diverse recommendations.
# 
# ## **Why are Recommender Systems important?**
# 
# ### Recommender Systems are crucial for various reasons:
# 
# ### **Personalization:** They enhance the user experience by offering tailored suggestions, saving users time and effort in finding items that match their preferences.
# 
# ### **Increased Engagement:** By presenting relevant and interesting content, Recommender Systems keep users engaged and coming back to the platform.
# 
# ### **Business Growth:** In e-commerce and online services, personalized recommendations can lead to increased sales, higher user retention, and customer loyalty.
# 
# ### **Discovery of New Items:** Recommender Systems can introduce users to new and lesser-known items they might not have discovered on their own.
# 
# ### **Decision Making Support:** They assist users in making informed decisions by suggesting items that align with their interests.
# 
# ### In summary, Recommender Systems are powerful tools that leverage data and algorithms to provide personalized recommendations, benefiting both users and businesses alike. Their ability to cater to individual preferences and offer a more enjoyable user experience makes them an essential part of modern applications and services.
# 

# ## Step-by-Step Explanation of a Content-Based Movie Recommender System:
# 
# 
# ### **Data Collection:** The first step is to gather data about movies and their attributes. This data could include information such as movie titles, genres, actors, directors, release years, and user ratings.
# 
# ### Data Preprocessing: Next, the collected data needs to be processed and cleaned to ensure its accuracy and consistency. For example, handling missing values, removing duplicates, and transforming the data into a suitable format.
# 
# ### Feature Extraction: In this step, we extract important features or characteristics from the movie data. These features act as the basis for the recommendations. For instance, we might convert movie genres into binary values, where each genre becomes a feature.
# 
# ### User Profile Creation: When a user interacts with the system, they provide information about their preferences, which are used to create a user profile. This profile contains their preferred movie features based on the movies they have liked or rated in the past.
# 
# ### Similarity Measurement: The system then compares the user's profile with the features of other movies in the database. This comparison is done using similarity measurement techniques, such as cosine similarity or Euclidean distance.
# 
# ### Recommendation Generation: Movies with features similar to the user's profile are considered to be more relevant. The system selects the top-rated or most similar movies and recommends them to the user.
# 
# ### Feedback Loop: As the user interacts with the recommended movies, their feedback is incorporated into the system to improve future recommendations. For example, if the user likes a recommended movie, the system may give more weight to the features of that movie in their profile for future recommrendations.
# 
# ### Hereku Deployment: After building the Content-Based Recommender System, you can deploy it on Heroku. Heroku is a platform that allows you to host and run web applications, including Recommender Systems, in the cloud. By deploying on Heroku, your Movie Recommender System becomes accessible to users over the internet, and they can get movie recommendations through a user-friendly web interface.ly web interface.

# ### Link to the dataset
# https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv

# In[88]:


import numpy as np
import pandas as pd


# In[89]:


movies = pd.read_csv('tmdb_5000_movies.csv')
credits =pd.read_csv('tmdb_5000_credits.csv')


# In[90]:


movies.head(1)


# In[91]:


credits.head(1)


# In[92]:


credits.head(1)['crew'].values


# ### We are going to merge the both dataframes movies and credits based on the title column

# In[93]:


movies = movies.merge(credits , on = 'title')


# In[94]:


movies


# In[95]:


movies.shape


# In[96]:


credits.shape


# ### Hence we get total of 23 columns. You may think why is it 23 not 24? The answer is simple. Since we join based on title so only one came to the merge one

# ### Let's find out which column to keep and which not to keep based on the our recommender system. For example budget is not the essential factor for recommendation as thsi doesnot means that people like movies which has high budget

# In[97]:


movies['original_language'].value_counts()


# ### This means original_language will not affect the output of recommendation

# In[98]:


movies.info()


# ### Here we are not taking any numerical columns in our model. After the theoritical analysis we are only taking columns genres,id ,keywords, title, overview, title, cast, crew as for the model

# In[99]:


movies = movies[['genres','movie_id', 'keywords','title','cast','crew','overview']]


# ### The double square brackets [[ ]] are commonly used when working with data structures like lists of lists or 2D arrays (e.g., NumPy arrays) to access multiple columns.but only one for single column

# In[100]:


movies.head()


# ### From this dataframe we will make a new dataframe only containing 3 columns containing movie_id, tile and tags

# ### Tags is formed by merging genres, keywords, cast, crew and overview together. To do this the overview column is taken and title , keyword is added to it. The top three cast and crew on each movies is also added to the overview column making it a paragraph. Before doing it all the columns will be made good removing the unwanted information.

# ### First we will do preprocessing to make the columns in order and second thing is to remove missing and duplicate datas

# In[101]:


movies.isnull().sum()


# ### Thus there are three rows in overview column where the datas are missing so i am gonna drop them

# In[102]:


movies.dropna(inplace=True)


# In[103]:


movies.isnull().sum()


# In[104]:


movies.duplicated().sum()


# In[105]:


movies.iloc[0].genres


# ### In Python, when working with pandas DataFrames, the iloc indexer is used to access rows and columns by their integer positions. The indexing is zero-based, meaning the first row or column has the position 0, the second has the position 1, and so on.

# ### The above genres is in inappropriate format and i want it to be in ['Action','Adventure','Fantasy'] format. To do it we will create a helper funcion.

# In[106]:


import ast


# ###  ast.literal_eval() is a safe way to evaluate literal expressions in Python and is often used when you need to parse and evaluate strings containing simple data structures, such as lists or dictionaries, without risking arbitrary code execution.

# In[107]:


def convert(obj):
    L= []
    for i in ast.literal_eval(obj):
        L.append(i['name'])
    return L


# ### Here's a closer look at what the code does step by step:
# 
# ### def convert(obj):: This line tells us that we're defining a special function called "convert." This function can take something as input (we call it "obj"), and it will do some magic with it.
# 
# ### L = []: We have an empty list called "L." It's like an empty bag where we can put things.
# 
# ### for i in ast.literal_eval(obj):: This part is like a loop, and it helps us take each thing from "obj" one by one. We look inside "obj" and find some stuff there.
# 
# ### L.append(i['name']): Now, for each thing we found in "obj," we take its name and put it in our bag ("L"). The name is like a special tag for each thing, telling us what it is.
# 
# ### return L: Finally, after putting all the names in our bag, we take the bag ("L") and give it back to you.
# 
# ### So, this "convert" function takes something called "obj" (maybe a special list), looks inside it to find some things, takes their names, and gives you back a new list containing all those names.
# 
# ### It's important to note that this code assumes that "obj" is a string representing a list of dictionaries, and each dictionary has a key called "name." The ast.literal_eval() function helps convert the string representation of the list into an actual Python list. 

# In[108]:


movies['genres'] = movies['genres'].apply(convert)


# In[109]:


movies.head()


# In[110]:


movies['keywords'] = movies['keywords'].apply(convert)


# In[111]:


movies.head()


# In[112]:


movies['cast'][0]


# ### Be careful there are names and character. We don't need the name of the character like bhai we need name like salman khan so we will take out top three name of the actor which is the name. For this we will edit the function we used earlier uing counter to count only top 3 actors and break the if else for more than three counts.

# In[113]:


def convert3(obj):
    L= []
    counter = 0
    for i in ast.literal_eval(obj):
        if counter != 3:
            L.append(i['name'])
            counter +=1
        else:
            break
    return L
        


# In[114]:


movies['cast'] = movies['cast'].apply(convert3)


# In[115]:


movies.head()


# In[116]:


movies['crew'][0]


# ### We only need to those name where job is director as we are only interested in the name of the director.

# ### For this we will create a function similar to the above one

# In[117]:


def fetch_director(obj):
    L = []
    for i in ast.literal_eval(obj):
        if i['job'] == 'Director':
            L.append(i['name'])
            break
    return L


# In[118]:


movies['crew'] =  movies['crew'].apply(fetch_director)


# In[119]:


movies.head()


# In[120]:


movies['overview'][0]


# In[121]:


movies['overview'].astype


# ### As the overview column is a string, i want to convert it to list so that i can combine all the columns in one columns as planned.

# In[122]:


movies['overview'] = movies['overview'].apply(lambda x:x.split())


# ### lambda x: x.split(): This is a lambda function that takes an input x, which represents each entry (string) in the 'overview' column. The function then performs the split() method on x. The split() method is used to split a string into a list of substrings based on whitespace characters (spaces).

# In[123]:


movies.head()


# ### Now we are almost there. One problem is there may be names like sam altman and sam mendes. So if we didn't join these two then this can create confusion to the model. It can predict sam altman movie in place of sam mendes. To  get rid of this problem we will remove all the space between the words.

# ### Let's see the example

# In[124]:


movies['genres'] = movies['genres'].apply(lambda x:[i.replace(" ","") for i in x])
movies['keywords'] = movies['keywords'].apply(lambda x:[i.replace(" ","") for i in x])
movies['cast'] = movies['cast'].apply(lambda x:[i.replace(" ","") for i in x])
movies['crew'] = movies['crew'].apply(lambda x:[i.replace(" ","") for i in x])


# ### movies['genres']: This part of the code selects the 'genres' column from the 'movies' DataFrame. It assumes that the 'genres' column contains lists of strings.
# 
# ### lambda x: ...: This is a lambda function that takes an input x, which represents each entry (list of strings) in the 'genres' column.
# 
# ### [i.replace(" ", "") for i in x]: Inside the lambda function, a list comprehension is used to process each element of the input list x (i.e., each string in the 'genres' list).
# 
# ### i.replace(" ", ""): For each string i in the list, the replace() method is used to remove spaces from the string. The method is called with two arguments: the first argument is the substring to be replaced (in this case, a space " "), and the second argument is an empty string "", which means removing the spaces.
# 
# ### The result of the list comprehension is a new list containing strings with spaces removed from each element.
# 
# ### The apply() method is used to apply the lambda function to each element of the 'genres' column. It iterates through each list in the column, applies the lambda function to it, and creates a new Series containing the updated lists.

# In[125]:


movies.head()


# ### Let's now merge all the columns to the new columns tags

# In[126]:


movies['tags'] = movies['overview'] + movies['genres'] + movies['keywords']+ movies['cast'] + movies['crew']


# In[127]:


movies.head()


# In[128]:


new_df = movies[['movie_id','title','tags']]


# In[129]:


new_df


# ### Let's convert the list of column tags to string

# In[130]:


new_df['tags'] = new_df['tags'].apply(lambda x:" ".join(x))


# ### new_df['tags']: This part of the code selects the 'tags' column from the DataFrame new_df.
# 
# ### lambda x: ...: This is a lambda function that takes an input x, which represents each entry (list of strings) in the 'tags' column.
# 
# ### " ".join(x): Inside the lambda function, the join() method is used to concatenate the elements of the list x into a single string. The method is called on the separator string " ", which is a space. It joins the elements of the list with spaces in between.
# 
# ### The result of the lambda function is a new Series containing the concatenated strings.
# 
# ### The apply() method is used to apply the lambda function to each element of the 'tags' column. It iterates through each list in the column, applies the lambda function to it, and creates a new Series containing the concatenated strings.

# In[131]:


new_df.head()


# In[132]:


new_df['tags'][0]


# ### Let's now convert it to the lowercase

# In[133]:


new_df['tags'] = new_df['tags'].apply(lambda x:x.lower())


# In[134]:


new_df.head()


# ### Now we are almost ready! Now our problem is we need to recommend the five movies to the user based on the choice. We will recommend based on the tags.

# ### Now we need to calculate the similarities between the movies so that we can recommend it to the user based on the tags.

# In[135]:


new_df['tags'][0]


# In[136]:


new_df['tags'][1]


# ### Based on the text of above two movies, we need to calculate the similarity score. This is the text data not a number. So one way to calculate the similarity score is to count for the same words in both tags of both movies. But this is not the good option. This is where victorization is needed.

# ### Now we will convert each tags to a vector. This means each movie will become a vector. This will allow us to make almost 5000 vectors. Thus the recommender system will look for most similar vectors to the origin vector to recommend. 

# # Text Vectorization

# ### Text vectorization in the context of machine learning refers to the process of converting textual data into numerical representations (vectors) that can be used as input for machine learning algorithms. Since most machine learning models work with numerical data, text vectorization is essential for processing and analyzing text-based information.
# 
# ### Text vectorization techniques aim to preserve the semantic meaning of the text while transforming it into a numerical format. The ultimate goal is to represent the text in a way that captures relevant information, such as word frequency, context, and relationships, to enable effective analysis and modeling.
# 
# ### There are several popular text vectorization techniques, including:
# 
# ## 1. Bag of Words (BoW):
# ### In the Bag of Words approach, the text is represented as an unordered collection of words (tokens), and each word is assigned a unique index. The vector representation consists of a count or frequency of each word in the document. The order of words is ignored, and only the occurrence or absence of words matters. This approach creates a sparse vector, especially for large vocabularies.
# 
# ### Example:

# ### Text 1: "Machine learning is fascinating."
# ### Text 2: "Machine learning is challenging."
# 
# ### Vocabulary: ["machine", "learning", "is", "fascinating", "challenging"]
# 
# ### BoW representation:
# ### Text 1: [1, 1, 1, 1, 0]
# ### Text 2: [1, 1, 1, 0, 1]
# 

# ## 2. Term Frequency-Inverse Document Frequency (TF-IDF):
# ### TF-IDF is a variation of the BoW approach that takes into account the importance of words in a document relative to the entire corpus. It calculates a weight for each word based on its frequency in the document (term frequency) and its rarity across the entire corpus (inverse document frequency). TF-IDF assigns higher weights to words that are important within a specific document but relatively rare across all documents.
# 
# ### Example:

# ### Text 1: "Machine learning is fascinating."
# ### Text 2: "Machine learning is challenging."
# 
# ### Vocabulary: ["machine", "learning", "is", "fascinating", "challenging"]
# 
# ### TF-IDF representation:
# ### Text 1: [0.38, 0.38, 0.38, 0.53, 0]
# ### Text 2: [0.38, 0.38, 0.38, 0, 0.53]
# 

# ## 3. Word Embeddings:
# ### Word embeddings are dense vector representations of words that are learned through neural network-based language models such as Word2Vec, GloVe, or FastText. These embeddings capture semantic relationships between words, making them valuable in tasks like word analogies and capturing context information.
# 
# ### Example:

# ### Word embeddings for "machine":
# ### [0.12, -0.45, 0.72, ...]
# 
# ### Word embeddings for "learning":
# ### [-0.33, 0.25, -0.16, ...]
# 

# ### Text vectorization is a fundamental step in natural language processing (NLP) and allows machine learning models to work effectively with text data, enabling tasks like sentiment analysis, text classification, machine translation, and more. The choice of text vectorization technique depends on the specific task and the characteristics of the dataset.
# 
# 
# 
# 
# 

# # How Bag of Words (BoW) works?

# ### The Bag of Words (BoW) model is a popular and simple text representation technique used in natural language processing (NLP) and machine learning. It converts a piece of text into a numerical vector representation, disregarding grammar and word order but considering the frequency of each word in the text. BoW is a fundamental step in text vectorization and allows us to perform various text-based tasks like text classification, sentiment analysis, and information retrieval.
# 
# ## Here's how the Bag of Words model works in detail:
# 
# ## Step 1: Corpus Creation
# ### A corpus is a collection of text documents, such as sentences, paragraphs, or entire books. The BoW model requires a corpus of text from which it constructs a vocabulary, a unique set of words present in the corpus.
# 
# ### Example Corpus:
# 
# ### Document 1: "The quick brown fox jumps over the lazy dog."
# ### Document 2: "A brown fox is running in the field."
# ### Document 3: "The dog and the fox are friends."
# ## Step 2: Tokenization
# ### Tokenization is the process of breaking down text into individual units, typically words or phrases (tokens). In the BoW model, we tokenize each document in the corpus, separating them into individual words.
# 
# ### Tokenization of Document 1:
# 
# 
# ### ["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]
# ## Step 3: Building the Vocabulary
# ### The vocabulary is created by collecting all unique words from the entire corpus. The size of the vocabulary depends on the total number of unique words present in the corpus.
# 
# ### Vocabulary:
# ### ["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog", "A", "is", "running", "in", "field", "and", "are", "friends"]
# 
# ## Step 4: Vectorization
# ### To represent each document in the corpus numerically, we create a vector of fixed length, equal to the size of the vocabulary. Each element of the vector corresponds to a word from the vocabulary, and its value is determined by the frequency of that word in the document.
# 
# ### Vector Representation of Document 1:
# 
# 
# ### [2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
# ### The vector indicates that the word "The" appears twice in Document 1, "quick," "brown," "fox," "jumps," "over," "the," "lazy," and "dog" appear once, while all other words have a frequency of zero.
# 
# ## Step 5: Representing the Entire Corpus
# ### After vectorizing all documents in the corpus, we obtain a matrix representation known as the Document-Term Matrix (DTM) or the BoW matrix. Each row of the matrix corresponds to a document, and each column represents a word from the vocabulary.
# 
# ### Document-Term Matrix (BoW Matrix):
# 
# 
# ###  |           | The|  quick|  brown  |fox  |jumps|  over  |the  |lazy  |dog | A | is  |running  |in | field | and  |are  |friends|
# ### |Doc1   |2 |   1 |   1   |    1  |  1   |   1  |    1   | 1   |   1  |  0  |0  | 0    |   0   |0    | 0   |  0   | 0    |   0|
# ### |Doc2   |0 |  0  |    1  |   1   |  0   |   0  |   0    |  0  |   0  |  1  |1  |  1   |     1 |  1  |  0  | 0    |  0   |   0|
# ### |Doc3   |1 | 0   |   0   |  1    |  0   |   0  |   2    | 0   |  1   |  0  |  0| 0    |    0  |0    | 1   | 1    |  1   |   1|
# ### Each row in the BoW matrix represents a document, and each column represents a word from the vocabulary. The values in the matrix indicate the frequency of each word in each document.
# 
# ### BoW is a simple yet effective approach for text representation. However, it ignores the context and word order, which may limit its performance on certain NLP tasks. Other advanced techniques like TF-IDF, word embeddings, and deep learning-based models aim to address these limitations while capturing more meaningful representations of textual data.

# ### First all the tags from all the movies is combined. The most frequently used words from the combined equal to the number of datas of movies is determined. This means if salmankhan is mostly appearing than it comes to the top frequently word. Now i will take all the words from the individual tag of the movie for example avatar. Now i will search the frequency of the top appearing words which was equal to the number of movie in the tags of individual movie we took. This will provide us the unique vector to us for all movies.

# ## Stop Word

# 
# ### In natural language processing (NLP), stop words are common words that are often removed from text data during preprocessing because they are considered to be of little value for most NLP tasks. These words are typically common and appear frequently across different documents, but they do not carry significant meaning or contribute much to the understanding of the text. Removing stop words can help reduce the dimensionality of the data, improve processing speed, and focus on the more important words in the text.
# 
# ### Examples of stop words in English include words like "the," "and," "is," "in," "of," "a," "an," "to," "that," "it," "for," "with," etc.

# In[137]:


new_df.head()


# In[138]:


from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 5000, stop_words = 'english')


# ### The CountVectorizer is a feature extraction technique used in natural language processing (NLP) and text mining tasks to convert a collection of text documents into numerical feature vectors. It creates a "bag of words" representation, where each document is represented as a vector of word frequencies. The CountVectorizer is part of the sklearn.feature_extraction.text module in scikit-learn, a popular machine learning library in Python.

# # #max_features (default=None):
# 
# ### max_features is an optional parameter that limits the total number of features (words or vocabulary) that the CountVectorizer will consider. It selects the top max_features most frequent words in the dataset based on their document frequency (i.e., how many documents contain each word).
# 
# ### If max_features is set to None (default), all words will be considered, and the size of the vocabulary will be equal to the number of unique words in the entire corpus. If set to an integer value (e.g., 5000), only the top 5000 most frequent words will be used as features, and less frequent words will be ignored.
# 
# ## stop_words (default=None):
# 
# ### stop_words is an optional parameter that allows you to specify a list of words to be removed from the text data before feature extraction. These words are referred to as "stop words" and typically include common and non-informative words like "the," "and," "is," "in," etc.
# 
# ### When set to 'english', scikit-learn uses its built-in list of English stop words. This removes common English stop words from the text data before vectorization.
# ### If set to None (default), no stop words will be removed.

# In[139]:


vectors = cv.fit_transform(new_df['tags']).toarray()


# In[140]:


cv.fit_transform(new_df['tags']).toarray().shape


# In[141]:


vectors[0]


# ### The CountVectorizer class in scikit-learn provides several useful methods for working with text data. These methods allow you to fit the vectorizer to the data, transform the text data into numerical feature vectors, and perform various operations related to text processing. Let's explore some of the most commonly used methods of the CountVectorizer:
# 
# ## fit(raw_documents, y=None):
# 
# ### This method learns the vocabulary from the raw text documents (raw_documents). It analyzes the text data and builds a mapping of words to feature indices. The fit() method must be called before using transform() or fit_transform().
# ### Parameters:
# ### raw_documents: An iterable of raw text documents, where each document is a string.
# ### y: Not used in CountVectorizer, it's only there to maintain the scikit-learn API.
# 
# ## transform(raw_documents):
# 
# ### This method transforms the raw text documents (raw_documents) into numerical feature vectors. It uses the vocabulary learned during the fit() step to encode the text data.
# ### Parameters:
# ### raw_documents: An iterable of raw text documents, where each document is a string.
# ### Note: If you want to perform both fitting and transforming in a single step, you can use the fit_transform() method, which is more efficient.
# 
# ## fit_transform(raw_documents, y=None):
# 
# ### This method combines the fit() and transform() operations into a single step. It learns the vocabulary from the raw text documents (raw_documents) and transforms the text data into numerical feature vectors using the learned vocabulary.
# ### Parameters:
# ### raw_documents: An iterable of raw text documents, where each document is a string.
# ### y: Not used in CountVectorizer, it's only there to maintain the scikit-learn API.
# ### This method returns the resulting feature matrix.
# 
# ## get_feature_names_out(input_features=None):
# 
# ### This method returns the list of feature names (words) in the vocabulary. It can be helpful to inspect the vocabulary after fitting the CountVectorizer.
# ### Parameters:
# ### input_features: Optional. A list of input feature names. If provided, this method will transform the feature names back into their original form. This is useful when using feature selection methods and you want to map the feature indices back to the actual words.
# 
# ## get_stop_words():
# 
# ### This method returns the stop words used by the CountVectorizer. If no stop words were specified during initialization, it will return None.
# 
# ## inverse_transform(X):
# 
# ### This method transforms the numerical feature matrix X back into raw text documents. It reconstructs the original text data from the bag of words representation.
# ### Parameters:
# ### X: A feature matrix obtained from transform().
# ### 
# ### These are some of the most commonly used methods of the CountVectorizer. In addition to these, the CountVectorizer also has other methods that provide information about its state and configuration, such as get_params(), set_params(), fit_transform_info(), and more. For more details and additional methods, refer to the official scikit-learn documentation:
# 
# ### CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

# In[142]:


cv.get_feature_names()


# In[143]:


len(cv.get_feature_names())


# ### I have some issues with the result. The feature activity and activities must be same also actions and action must be same. I too have problem with the numbers but numbers like 1930s can give some meaning.

# # Stemming

# ### Stemming is a text processing technique used in natural language processing (NLP) to reduce words to their base or root form, known as the "stem" or "root" word. The purpose of stemming is to normalize words by removing suffixes and prefixes so that words with the same root but different inflections are mapped to the same stem. This helps in reducing the dimensionality of the text data and grouping similar words together, making it easier for further text analysis and processing.
# 
# ### The stemming process involves applying specific rules or algorithms to words to derive their stems. The resulting stems may not always be valid words in the language, but they serve as a common representation for related words. Since stemming is a heuristic approach, it may sometimes produce imperfect results, but it is computationally efficient and widely used in various NLP tasks.
# 
# ### For example, consider the following words:
# 
# ### jumps
# ### jumping
# ### jumped
# ### After stemming, these words would be reduced to their common root "jump."
# 
# ### Popular stemming algorithms include:
# 
# ## Porter Stemmer: 
# ### This is one of the most widely used stemming algorithms. It applies a series of rules to strip suffixes and bring words to their base form.
# 
# ## Snowball Stemmer (Porter2): 
# ### An improved version of the Porter Stemmer that supports multiple languages and has better performance.
# 
# ## Lancaster Stemmer: 
# ### A more aggressive stemming algorithm compared to the Porter Stemmer, resulting in shorter stems.
# 
# ### In Python, the NLTK (Natural Language Toolkit) library provides implementations of various stemming algorithms, including the Porter Stemmer and the Snowball Stemmer. 

# In[144]:


import nltk


# In[145]:


from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()


# ### Let's create a helper function to do it|

# In[146]:


def stem(text):
    y = []
    for i in text.split():
        y.append( ps.stem(i))
    return " ".join(y)


# ## def stem(text):
# ### This line defines a Python function called stem that takes a single argument text.
# 
# ## y = []
# ### This line initializes an empty list called y. This list will be used to store the stemmed words.
# 
# ## for i in text.split():
# ### This line starts a loop that iterates through each word in the input text. The text.split() function splits the input text into individual words, and the loop will process each word one by one.
# 
# ## y.append(ps.stem(i))
# ### inside the loop, the line applies stemming to the current word i using the PorterStemmer instance ps. The stemmed word is then appended to the list y.
# 
# ## return " ".join(y)
# ### After processing all the words in the input text, this line joins the stemmed words in the list y back into a single string. The join() function concatenates the words with spaces between them, effectively reconstructing the stemmed text.

# In[147]:


ps.stem('loving')


# In[148]:


new_df['tags'] = new_df['tags'].apply(stem)


# In[149]:


vectors = cv.fit_transform(new_df['tags']).toarray()


# In[150]:


cv.get_feature_names()


# In[151]:


len(cv.get_feature_names())


# ### We have 4806 movies this means we also have 4806 vectors and in every vectors there are 5000 numbers.Now we need to calculate the distance between each vectors. Higher distance means less similarity and vice versa. We will not calculate euclidian distance rather we will calculate the cosine distance since the euclidean distance fails in higher dimensional space.

# ### In the context of data analysis and machine learning, distance metrics are used to quantify the similarity or dissimilarity between two data points or vectors. There are several types of distance metrics, each suited for different types of data and applications. Below are some commonly used distance metrics:
# 
# ## Euclidean Distance:
# ### Euclidean distance is the most common distance metric in Euclidean space (commonly known as "flat" or "Cartesian" space).
# ### It calculates the straight-line distance between two points and is defined as the square root of the sum of squared differences between corresponding elements of the two vectors.
# ### It is commonly used in clustering, k-nearest neighbors, and other machine learning algorithms.
# ### Formula: sqrt(sum((x_i - y_i)^2))
# 
# ## Manhattan Distance (City Block or L1 Distance):
# ### Manhattan distance calculates the distance between two points by summing the absolute differences of their corresponding elements.
# ### It measures the distance along the axis-aligned grid or "city blocks" in a grid-like pattern.
# ### It is often used when dimensions have different units or scales.
# ### Formula: sum(|x_i - y_i|)
# 
# ## Cosine Distance (Cosine Similarity):
# ### Cosine distance measures the cosine of the angle between two vectors.
# ### It is used for text analysis and other high-dimensional data to capture the direction rather than the magnitude of the vectors.
# ### It ranges from -1 (completely dissimilar) to 1 (completely similar).
# ### Formula: 1 - (dot product(x, y) / (norm(x) * norm(y)))
# 
# ## Jaccard Distance (Jaccard Similarity):
# ### Jaccard distance is used for comparing sets. It calculates the dissimilarity between two sets by dividing the size of their intersection by the size of their union.
# ### It is commonly used in text analysis for comparing the similarity of two documents based on their word sets.
# ### Formula: 1 - (|A ∩ B| / |A ∪ B|)
# 
# ## Hamming Distance:
# ### Hamming distance is used for comparing two binary strings of equal length.
# ### It measures the number of positions at which the corresponding bits are different.
# ### It is widely used in error detection and correction codes.
# ### Formula: Number of differing bits
# 
# ## Mahalanobis Distance:
# ### Mahalanobis distance measures the distance between a data point and the center of a dataset, taking into account the correlation between different features.
# ### It is used when the dataset has correlated features and is sensitive to data scale and correlation.
# ### Formula: sqrt((x - μ)^T * Σ^(-1) * (x - μ))
# 
# ### These are just a few examples of distance metrics. The choice of distance metric depends on the nature of the data and the specific problem at hand. Selecting an appropriate distance metric is crucial for accurate and meaningful results in various machine learning tasks such as clustering, classification, and recommendation systems.

# In[152]:


from sklearn.metrics.pairwise import cosine_similarity 


# ### The cosine_similarity function from sklearn.metrics.pairwise is a part of the scikit-learn library in Python. It is used to calculate the cosine similarity between pairs of data points represented as vectors or matrices. As mentioned earlier, cosine similarity is a distance metric used to measure the similarity between two vectors in terms of their orientation or direction, rather than their magnitude. It is particularly useful in high-dimensional data and text analysis.
# 
# ### The cosine_similarity function works similarly to the cosine_similarity function from sklearn.metrics. However, it is specifically designed to handle pair-wise similarity calculations efficiently and is commonly used in applications such as text analysis, collaborative filtering, and recommendation systems.klearn.metrics.pairwise import cosine_similarity
# 
# ### Parameters:
# ### X: The first input array or sparse matrix containing the vectors.
# ### Y: The second input array or sparse matrix containing the vectors. If not provided, the function will calculate the similarity between vectors in X.
# ### The function returns a similarity matrix, which represents the pairwise cosine similarity between all pairs of vectors. The shape of the output matrix will be (n_samples_X, n_samples_Y), where n_samples_X is the number of vectors in the first input array and n_samples_Y is the number of vectors in the second input array.

# In[153]:


similarity = cosine_similarity(vectors)


# In[154]:


cosine_similarity(vectors).shape


# In[155]:


similarity


# In[156]:


similarity[0]


# ### This is the distance of first movie with all other movies. The distance of first movie with itself is 1.

# ### Now i will create a funciton to recommend movie. first i need to get the index of that movie, after getting index i will enter into similarity matrix.Then i will sort distance stored in the matrix. This will provide me the most similar movie.

# ### Let's find the index

# In[157]:


new_df['title']== 'Avatar'


# In[158]:


new_df[new_df['title']== 'Avatar']


# In[159]:


new_df[new_df['title']== 'Avatar'].index


# In[160]:


new_df[new_df['title']== 'Avatar'].index[0]


# In[161]:


new_df[new_df['title']== 'Aliens'].index[0]


# ### this is the index of the movie "Aliens"

# In[162]:


sorted(similarity[0])


# ###  This is the sorting in ascending order. Let's do it in descending order

# In[163]:


sorted(similarity[0])[-10:-1]


# In[164]:


sorted(similarity[0] , reverse = True)


# ### during this sorting process i am losing the index position. For this we need to call enumerate function.
# 
# ### The enumerate() function is a built-in Python function that allows you to iterate over a sequence (such as a list, tuple, string, or other iterable) while keeping track of the index of the current item. It returns an iterator that generates pairs of (index, item) for each element in the sequence.
# 
# ### The enumerate() function is particularly useful when you need to perform some action based on the index or when you want to create dictionaries or other data structures based on the index-item relationship of a sequence. It is a handy tool for various scenarios where you require access to both the item and its index in a loop.

# In[165]:


list(enumerate(similarity[0]))


# ### This is the distance of first movie with first one and first movie with second one. Here enumerate function makes the pair of index on first and distances on second. To sort on the basis of distances now we need to sort based on the second elements on the matrix. We are sorting in descending order.

# In[166]:


sorted(list(enumerate(similarity[0],)) ,reverse = True, key = lambda x:x[1])


# ### Now i only want to get top 5 movies from second to sixth

# In[167]:


sorted(list(enumerate(similarity[0],)) ,reverse = True, key = lambda x:x[1])[1:6]


# In[168]:


new_df.iloc[1216]


# In[169]:


new_df.iloc[1216].title


# ### From the combination of all the all the above codes, we create the function below.

# In[170]:


def recommend(movie):
    movies_index = new_df[new_df['title'] == movie].index[0]
    distances = similarity[movies_index]
    movies_list = sorted(list(enumerate(distances)),reverse = True, key =lambda x:x[1])[1:6]
    
    for i in movies_list:
        print(new_df.iloc[i[0]].title)


# In[171]:


recommend('Avatar')


# ### Here is a more detailed explanation of each step:
# 
# ## Find the index of the input movie in the DataFrame:
# 
# ### The new_df DataFrame contains information about movies, and we want to recommend similar movies based on cosine similarity. To do that, we need to find the index of the input movie in the DataFrame new_df. This is done using the line:
# 
# ### movie_index = new_df[new_df['title'] == movie].index[0]
# 
# ### This line uses boolean indexing to filter rows in new_df where the 'title' column matches the input movie name. The .index[0] is used to extract the index value of the first matching movie (assuming there is only one match).
# 
# ## Calculate the cosine similarity between the input movie and all other movies:
# 
# ### The similarity variable contains a pre-calculated matrix of cosine similarities between all pairs of movies. similarity[movie_index] extracts the row corresponding to the input movie from the similarity matrix. This gives us an array of cosine similarity scores between the input movie and all other movies in the DataFrame.
# 
# ### distances = similarity[movie_index]
# 
# ### Now, the distances array holds the cosine similarity scores of the input movie with all other movies in the DataFrame.
# 
# ## Create a list of (movie_index, cosine_similarity) pairs and sort it:
# 
# ### In this step, we create a list called movies_list that contains tuples of the form (movie_index, cosine_similarity) for each movie in the DataFrame (excluding the input movie itself). The list comprehension accomplishing this is:
# 
# ### movies_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]
# 
# ### The enumerate(distances) function pairs each cosine similarity score with its corresponding movie index.
# ### sorted() is used to sort the list in descending order based on the cosine similarity values (i.e., in decreasing order of similarity).
# ### [1:6] slices the top 5 recommended movies (excluding the input movie itself since it has the highest cosine similarity with itself).
# 
# ## Print the titles of the top 5 recommended movies:
# 
# ### Finally, we loop through the movies_list, which contains the top 5 movie indices and their cosine similarity scores. We then use these indices to access the movie titles in the DataFrame new_df and print them as the recommended movies.
# 
# ### for i in movies_list:
#    ### print(new_df.iloc[i[0]].title)
# 
# ### Here, new_df.iloc[i[0]] retrieves the movie row in the DataFrame using the movie index i[0]. Then, title is used to access the movie's title and print it as a recommended movie.
# ### The recommend(movie) function effectively takes an input movie, calculates its similarity with all other movies, and then returns the titles of the top 5 recommended movies based on cosine similarity. These recommended movies are likely to be similar in content or theme to the input movie.

# 
# 
# 
# ### This is all for the model building. Now we will convert this entire things in to the website. 
# ### for this we will use PyCharm.

# In[172]:


new_df


# In[173]:


import pickle


# In[174]:


pickle.dump(new_df,open('movies.pkl','wb'))


# ### pickle: The pickle module in Python is used for object serialization and deserialization. It allows you to convert Python objects (like data structures, variables, etc.) into a format that can be stored in a file or transferred over a network. This process is known as pickling.
# ### dump(): The dump() function is part of the pickle module. It is used to serialize an object and save it to a file.
# ### pickle: It's a Python module that helps save Python objects (like data structures) to files and load them back later.
# 
# ### new_df: This is the DataFrame object that contains some data. We want to save this DataFrame to a file.
# 
# ### 'movies.pkl': The name of the file where the DataFrame will be saved. The file will have the .pkl extension, which is common for pickled files.
# 
# ### 'wb': It specifies how the file should be opened. In this case, it means the file will be opened in "write binary" mode, which is needed when saving binary data like the pickled DataFrame

# In[177]:


pickle.dump(new_df.to_dict(),open('movies_dict.pkl','wb'))


# ### In Python, the to_dict() method is used to convert a Pandas DataFrame or Series object into a Python dictionary. It is a built-in method provided by the Pandas library, specifically for DataFrame and Series objects, and it allows you to transform tabular data into a dictionary-based representation.
# 
# ### When used with a DataFrame or Series, the to_dict() method generates a dictionary where the keys represent the column labels (for DataFrames) or index labels (for Series), and the values represent the data within the DataFrame or Series

# In[178]:


pickle.dump(similarity,open('similarity.pkl','wb'))


# In[ ]:




